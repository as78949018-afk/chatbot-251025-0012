# app.py
import os
import io
import json
import time
import math
import numpy as np

import streamlit as st
import openai as openai_module
from openai import OpenAI

# PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ: PyPDF2ê°€ ì—†ìœ¼ë©´ TXTë§Œ ì§€ì›
try:
    import PyPDF2
    HAS_PYPDF2 = True
except Exception:
    HAS_PYPDF2 = False

# ----------------------------
# ê¸°ë³¸ ì„¸íŒ…
# ----------------------------
st.set_page_config(page_title="ğŸ’¬ Chatbot (All-in-One)", page_icon="ğŸ’¬", layout="wide")
st.title("ğŸ’¬ Chatbot (All-in-One)")

with st.expander("ì„¤ëª… ë³´ê¸°", expanded=False):
    st.markdown(
        "- OpenAI ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. API í‚¤ëŠ” ì„¸ì…˜ì—ì„œë§Œ ì“°ì´ê³  ì„œë²„ì— ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
        "- ë°°í¬ ì‹œ **í™˜ê²½ë³€ìˆ˜** ë˜ëŠ” **Streamlit Secrets** ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n"
        "- ì—…ë¡œë“œ íŒŒì¼(PDF/TXT)ì€ ì„¸ì…˜ ë©”ëª¨ë¦¬ì—ë§Œ ì €ì¥ë©ë‹ˆë‹¤."
    )

# ----------------------------
# ì‚¬ì´ë“œë°”: ì„¤ì •
# ----------------------------
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    default_key = os.environ.get("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")
    openai_api_key = default_key or st.text_input("OpenAI API Key", type="password", help="í™˜ê²½ë³€ìˆ˜/Secretsê°€ ì—†ìœ¼ë©´ ì—¬ê¸°ì— ì…ë ¥")
    model = st.selectbox(
        "Model",
        ["gpt-4o", "gpt-4o-mini"],
        index=0,
        help="ì¼ë°˜ ëŒ€í™”: gpt-4o / ë¹„ìš©ì ˆê°: gpt-4o-mini"
    )
    temperature = st.slider("Temperature(ì°½ì˜ì„±)", 0.0, 1.2, 0.7, 0.1)
    max_output_tokens = st.slider("Max output tokens(ì‘ë‹µ ê¸¸ì´)", 64, 4096, 1024, 64)
    stream_enable = st.toggle("ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°", value=True, help="ë„ë©´ ì‘ë‹µ í›„ ì‚¬ìš©ëŸ‰(í† í°)ì„ í‘œì‹œí•©ë‹ˆë‹¤.")
    st.divider()

    st.subheader("Assistant ìŠ¤íƒ€ì¼ í”„ë¦¬ì…‹")
    preset = st.selectbox(
        "ë§íˆ¬/ì—­í•  í”„ë¦¬ì…‹",
        ["ê¸°ë³¸", "ì¹œì ˆí•œ íŠœí„°", "ì´ˆê°„ë‹¨ ìš”ì•½ë´‡", "ë¬¸ì¥ ë‹¤ë“¬ê¸°(êµì •)"],
        index=0
    )
    preset_map = {
        "ê¸°ë³¸": "You are a helpful, concise assistant.",
        "ì¹œì ˆí•œ íŠœí„°": "You are a friendly tutor. Explain step-by-step with small, clear paragraphs and examples.",
        "ì´ˆê°„ë‹¨ ìš”ì•½ë´‡": "You summarize any input into 3 bullet points with the most essential facts only.",
        "ë¬¸ì¥ ë‹¤ë“¬ê¸°(êµì •)": "Rewrite the user's text with improved clarity, grammar, and natural tone while preserving meaning."
    }
    system_prompt = st.text_area(
        "System prompt(ì„¸ë¶€ ì¡°ì • ê°€ëŠ¥)",
        value=preset_map.get(preset, preset_map["ê¸°ë³¸"]),
        height=100
    )

    st.subheader("ëŒ€í™” ê´€ë¦¬")
    max_turns_keep = st.slider("íˆìŠ¤í† ë¦¬ ë³´ì¡´ í„´(ì§ˆë¬¸/ë‹µë³€ ìŒ)", 5, 60, 30, 1)
    reset = st.button("ğŸ”„ ìƒˆ ëŒ€í™” ì‹œì‘")
    st.caption("ë„ˆë¬´ ê¸¸ì–´ì§€ë©´ ë¹„ìš©â†‘/ì†ë„â†“ â†’ ì˜¤ë˜ëœ ê¸°ë¡ì€ ìë™ íŠ¸ë¦¼")

# ----------------------------
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# ----------------------------
if "messages" not in st.session_state or reset:
    st.session_state.messages = []  # [{"role": "system"/"user"/"assistant", "content": "..."}]
    st.session_state.has_system = False

# ê°„ë‹¨ RAGìš© ì„¸ì…˜ ìƒíƒœ
if "rag_ready" not in st.session_state:
    st.session_state.rag_ready = False
if "rag_chunks" not in st.session_state:
    st.session_state.rag_chunks = []
if "rag_embeds" not in st.session_state:
    st.session_state.rag_embeds = None  # np.array shape (N, D)
if "rag_model" not in st.session_state:
    st.session_state.rag_model = "text-embedding-3-small"

# ----------------------------
# ë„ìš°ë¯¸ í•¨ìˆ˜
# ----------------------------
def ensure_system_message(prompt_text: str):
    """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ messages[0]ì— ë³´ì¥/ë™ê¸°í™”"""
    if not st.session_state.has_system:
        st.session_state.messages.insert(0, {"role": "system", "content": prompt_text})
        st.session_state.has_system = True
    else:
        # ì´ë¯¸ systemì´ ìˆë‹¤ë©´ ë‚´ìš© ì—…ë°ì´íŠ¸
        st.session_state.messages[0]["content"] = prompt_text

def trim_history(max_turns: int):
    """system + ìµœê·¼ max_turns*2 ìœ ì§€"""
    msgs = st.session_state.messages
    if not msgs:
        return
    sys = msgs[0] if msgs and msgs[0]["role"] == "system" else None
    body = msgs[1:] if sys else msgs[:]
    limit = max_turns * 2
    if len(body) > limit:
        body = body[-limit:]
    st.session_state.messages = ([sys] if sys else []) + body

def extract_text_from_pdf(file_bytes: bytes) -> str:
    """PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ(PYPDF2 í•„ìš”). ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´."""
    if not HAS_PYPDF2:
        return ""
    try:
        reader = PyPDF2.PdfReader(io.BytesIO(file_bytes))
        texts = []
        for page in reader.pages:
            t = page.extract_text() or ""
            texts.append(t)
        return "\n".join(texts)
    except Exception:
        return ""

def chunk_text(text: str, chunk_size: int = 900, overlap: int = 200):
    """ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¡œ í…ìŠ¤íŠ¸ ì²­í¬í™” (ê°„ë‹¨ ë²„ì „)"""
    text = " ".join(text.split())
    if not text:
        return []
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        if end >= len(text):
            break
        start = end - overlap
        if start < 0:
            start = 0
    return chunks

def cosine_sim(A: np.ndarray, B: np.ndarray):
    """A: (n,d), B:(m,d) -> (n,m) ì½”ì‚¬ì¸ ìœ ì‚¬ë„"""
    # ì •ê·œí™”
    A_norm = A / (np.linalg.norm(A, axis=1, keepdims=True) + 1e-12)
    B_norm = B / (np.linalg.norm(B, axis=1, keepdims=True) + 1e-12)
    return A_norm @ B_norm.T

def build_embeddings(client: OpenAI, chunks: list[str], embed_model: str) -> np.ndarray:
    """OpenAI ì„ë² ë”© ìƒì„± -> np.array (N, D)"""
    if not chunks:
        return np.zeros((0, 1536), dtype=np.float32)
    resp = client.embeddings.create(
        model=embed_model,
        input=chunks
    )
    vecs = [d.embedding for d in resp.data]
    return np.array(vecs, dtype=np.float32)

def retrieve_context(query: str, top_k: int = 4) -> str:
    """ì§ˆì˜ì™€ ê°€ì¥ ìœ ì‚¬í•œ ì²­í¬ top_kë¥¼ ì´ì–´ë¶™ì—¬ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
    if not (st.session_state.rag_ready and st.session_state.rag_embeds is not None):
        return ""
    try:
        client = OpenAI(api_key=openai_api_key)
        q_embed = client.embeddings.create(
            model=st.session_state.rag_model, input=[query]
        ).data[0].embedding
        q_vec = np.array([q_embed], dtype=np.float32)  # (1, D)
        sims = cosine_sim(q_vec, st.session_state.rag_embeds).flatten()  # (N,)
        idx = np.argsort(-sims)[:top_k]
        selected = [st.session_state.rag_chunks[i] for i in idx]
        context = "\n\n".join(selected)
        return context
    except Exception:
        return ""

def export_chat_as_txt(messages: list[dict]) -> bytes:
    lines = []
    for m in messages:
        role = m.get("role", "")
        if role == "system":
            continue
        content = m.get("content", "")
        lines.append(f"[{role.upper()}]\n{content}\n")
    return "\n".join(lines).encode("utf-8")

def export_chat_as_json(messages: list[dict]) -> bytes:
    # systemë„ í•¨ê»˜ ë‚´ë³´ë‚´ê³  ì‹¶ìœ¼ë©´ í•„í„° ì œê±°
    payload = [m for m in messages if m.get("role") in ("user", "assistant")]
    return json.dumps(payload, ensure_ascii=False, indent=2).encode("utf-8")

# ----------------------------
# API í‚¤ ì²´í¬
# ----------------------------
if not openai_api_key:
    st.info("ì¢Œì¸¡ ì‚¬ì´ë“œë°”ì—ì„œ OpenAI API Keyë¥¼ ì…ë ¥í•˜ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜/Secretsë¥¼ ì„¤ì •í•˜ì„¸ìš”. ğŸ”", icon="ğŸ—ï¸")
    st.stop()

client = OpenAI(api_key=openai_api_key)

# ----------------------------
# íŒŒì¼ ì—…ë¡œë“œ â†’ RAG ì¸ë±ìŠ¤ ë§Œë“¤ê¸°
# ----------------------------
st.subheader("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ (ì„ íƒ) â€” PDF/TXT ì§€ì›, ì§ˆì˜ ì‘ë‹µì— í™œìš©")
uploaded_files = st.file_uploader(
    "ì—¬ê¸°ì— PDFë‚˜ TXTë¥¼ ì˜¬ë¦¬ë©´, ë‚´ìš© ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ í’ˆì§ˆì´ í–¥ìƒë¼ìš”. ì—¬ëŸ¬ íŒŒì¼ ê°€ëŠ¥.",
    type=["pdf", "txt"],
    accept_multiple_files=True
)

build_btn_cols = st.columns([1, 1, 6])
with build_btn_cols[0]:
    use_rag = st.toggle("RAG ì‚¬ìš©", value=False, help="ì¼œë©´ ì—…ë¡œë“œí•œ íŒŒì¼ ë‚´ìš©ì´ ë‹µë³€ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.")
with build_btn_cols[1]:
    rebuild = st.button("ğŸ“š ì¸ë±ìŠ¤ ìƒì„±/ì¬ìƒì„±")

if rebuild and uploaded_files:
    all_text = []
    for f in uploaded_files:
        if f.type == "text/plain" or f.name.lower().endswith(".txt"):
            text = f.read().decode("utf-8", errors="ignore")
        elif f.type == "application/pdf" or f.name.lower().endswith(".pdf"):
            if not HAS_PYPDF2:
                st.warning(f"'{f.name}' â†’ PyPDF2 ë¯¸ì„¤ì¹˜ë¡œ PDF ì¶”ì¶œ ë¶ˆê°€(TXTë§Œ ì§€ì›).")
                text = ""
            else:
                text = extract_text_from_pdf(f.read())
        else:
            text = ""
        if text:
            all_text.append(text)

    full_text = "\n\n".join(all_text)
    chunks = chunk_text(full_text, chunk_size=900, overlap=200)

    if not chunks:
        st.warning("ì¶”ì¶œ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ìŠ¤ìº” PDFëŠ” í…ìŠ¤íŠ¸ê°€ ì—†ì„ ìˆ˜ ìˆì–´ìš”.")
    else:
        with st.spinner("ì„ë² ë”© ìƒì„± ì¤‘..."):
            vecs = build_embeddings(client, chunks, st.session_state.rag_model)
        st.session_state.rag_chunks = chunks
        st.session_state.rag_embeds = vecs
        st.session_state.rag_ready = True
        st.success(f"ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ! ì²­í¬ {len(chunks)}ê°œ")

# ----------------------------
# ê¸°ì¡´ íˆìŠ¤í† ë¦¬ ë Œë”ë§
# ----------------------------
for m in st.session_state.messages:
    if m["role"] in ("user", "assistant"):
        with st.chat_message(m["role"]):
            st.markdown(m["content"])

# ----------------------------
# ëŒ€í™” ì…ë ¥ + ì‘ë‹µ
# ----------------------------
user_input = st.chat_input("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? (Shift+Enter ì¤„ë°”ê¿ˆ)")
if user_input:
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë³´ì¥ & íˆìŠ¤í† ë¦¬ íŠ¸ë¦¼
    ensure_system_message(system_prompt)
    trim_history(max_turns_keep)

    # ì‚¬ìš©ì ë°œí™” í‘œì‹œ/ì €ì¥
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # RAG ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    additional_context = ""
    if use_rag:
        ctx = retrieve_context(user_input, top_k=4)
        if ctx:
            additional_context = (
                "You may use the following context extracted from the user's documents. "
                "If the context is relevant, ground your answer in it. If not, ignore it.\n\n"
                f"=== BEGIN CONTEXT ===\n{ctx}\n=== END CONTEXT ==="
            )

    # ëª¨ë¸ í˜¸ì¶œ
    try:
        # ë©”ì‹œì§€ êµ¬ì„± (RAG ì»¨í…ìŠ¤íŠ¸ëŠ” ì¶”ê°€ user ë©”ì‹œì§€ë¡œ ì „ë‹¬)
        call_messages = list(st.session_state.messages)
        if additional_context:
            call_messages.append({"role": "user", "content": additional_context})

        if stream_enable:
            # ìŠ¤íŠ¸ë¦¬ë°
            with st.chat_message("assistant"):
                stream = client.chat.completions.create(
                    model=model,
                    messages=[{"role": m["role"], "content": m["content"]} for m in call_messages],
                    temperature=temperature,
                    max_tokens=max_output_tokens,
                    stream=True,
                )
                response_text = st.write_stream(stream)
            st.session_state.messages.append({"role": "assistant", "content": response_text})
        else:
            # ë¹„ìŠ¤íŠ¸ë¦¬ë°(usage í‘œì‹œ)
            with st.chat_message("assistant"):
                resp = client.chat.completions.create(
                    model=model,
                    messages=[{"role": m["role"], "content": m["content"]} for m in call_messages],
                    temperature=temperature,
                    max_tokens=max_output_tokens,
                    stream=False,
                )
                response_text = resp.choices[0].message.content
                st.markdown(response_text)
                if getattr(resp, "usage", None):
                    in_tok = resp.usage.prompt_tokens
                    out_tok = resp.usage.completion_tokens
                    tot_tok = resp.usage.total_tokens
                    st.caption(f"ğŸ§® tokens â€” prompt: {in_tok}, completion: {out_tok}, total: {tot_tok}")
            st.session_state.messages.append({"role": "assistant", "content": response_text})

    except openai_module.AuthenticationError:
        st.error("API í‚¤ ì¸ì¦ ì˜¤ë¥˜ì…ë‹ˆë‹¤. í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”. ğŸ”‘")
    except openai_module.RateLimitError:
        st.warning("ìš”ì²­ì´ ë§ì•„ ì ì‹œ ëŒ€ê¸°í•´ì•¼ í•©ë‹ˆë‹¤. â³")
    except openai_module.APIError as e:
        st.error(f"OpenAI API ì˜¤ë¥˜: {e}")
    except Exception as e:
        st.exception(e)

# ----------------------------
# ë‚´ë³´ë‚´ê¸°(ë‹¤ìš´ë¡œë“œ)
# ----------------------------
st.divider()
st.subheader("â¬‡ï¸ ëŒ€í™” ë‚´ë³´ë‚´ê¸°")
col_txt, col_json = st.columns(2)
with col_txt:
    st.download_button(
        "TXTë¡œ ì €ì¥",
        data=export_chat_as_txt(st.session_state.messages),
        file_name="chat_export.txt",
        mime="text/plain",
        use_container_width=True
    )
with col_json:
    st.download_button(
        "JSONìœ¼ë¡œ ì €ì¥",
        data=export_chat_as_json(st.session_state.messages),
        file_name="chat_export.json",
        mime="application/json",
        use_container_width=True
    )

# í‘¸í„° ë„ì›€ë§
st.caption(
    "Tip: ìŠ¤íŠ¸ë¦¬ë°ì„ ë„ë©´(ì‚¬ì´ë“œë°”) ì‘ë‹µ í›„ ì‚¬ìš©ëŸ‰(í† í°)ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. "
    "PDF í…ìŠ¤íŠ¸ ì¶”ì¶œì€ ë¬¸ì„œ ìœ í˜•ì— ë”°ë¼ ì¼ë¶€ ì œí•œë  ìˆ˜ ìˆì–´ìš”."
)
